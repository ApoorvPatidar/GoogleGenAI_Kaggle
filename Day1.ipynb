{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a really smart toy robot, but it doesn't know anything at first. ü§ñ\n",
      "\n",
      "AI is like teaching that robot how to do things all by itself, just by showing it lots and lots of examples.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "*   **Let's say you want to teach the robot to recognize cats.** üê±\n",
      "*   You show it tons and tons of pictures of cats: fluffy cats, sleepy cats, black cats, white cats.\n",
      "*   The robot looks at all those pictures and starts to notice patterns: pointy ears, whiskers, a furry tail.\n",
      "*   After seeing enough pictures, the robot learns that if something has those patterns, it's probably a cat!\n",
      "\n",
      "**So, AI is basically:**\n",
      "\n",
      "1.  **Giving a computer lots of information (like pictures or words).**\n",
      "2.  **The computer finding patterns in that information.**\n",
      "3.  **Using those patterns to make guesses, solve problems, or do things on its own.**\n",
      "\n",
      "**Examples of AI you might see:**\n",
      "\n",
      "*   **When your tablet suggests words as you type.** It's learned from other people's writing.\n",
      "*   **When a video game character knows how to react to what you do.** It's been programmed to learn how to play the game well.\n",
      "*   **When your parents use a GPS in the car.** The GPS uses AI to figure out the best route to take.\n",
      "\n",
      "**Is AI perfect?** Nope! Just like learning to ride a bike, AI sometimes makes mistakes. But the more it learns, the better it gets!\n",
      "\n",
      "**In short, AI is like teaching a computer to be smart by showing it lots of examples and letting it learn from them!** üí°\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key = GEMINI_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    contents=\"Explain AI to me like I am kid\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There's no single \"best\" season to visit Europe, as it highly depends on your priorities, interests, and tolerance for crowds and weather. Here's a breakdown of the pros and cons of each season:\n",
       "\n",
       "**Spring (April - May):**\n",
       "\n",
       "*   **Pros:**\n",
       "    *   **Pleasant Weather:** Mild temperatures, sunny days (especially in Southern Europe).\n",
       "    *   **Blooming Scenery:** Flowers are in full bloom, landscapes are vibrant.\n",
       "    *   **Fewer Crowds:** Tourist crowds are generally smaller than in summer.\n",
       "    *   **Lower Prices:** Accommodation and flights can be more affordable than in peak season.\n",
       "    *   **Festivals:** Many spring festivals and cultural events.\n",
       "\n",
       "*   **Cons:**\n",
       "    *   **Unpredictable Weather:** Can be rainy or chilly, especially in early spring and in Northern Europe.\n",
       "    *   **Some Attractions May Have Limited Hours:** Especially in smaller towns or off-season destinations.\n",
       "\n",
       "**Summer (June - August):**\n",
       "\n",
       "*   **Pros:**\n",
       "    *   **Warmest Weather:** Best for swimming, sunbathing, and outdoor activities.\n",
       "    *   **Long Daylight Hours:** More time to explore.\n",
       "    *   **Festivals and Events:** Numerous summer festivals, concerts, and outdoor events.\n",
       "    *   **All Attractions Open:** Full opening hours for museums, attractions, and tours.\n",
       "\n",
       "*   **Cons:**\n",
       "    *   **Peak Season Crowds:** Expect long lines and crowded tourist spots.\n",
       "    *   **Highest Prices:** Accommodation and flights are typically the most expensive.\n",
       "    *   **Heat Waves:** Southern Europe can experience intense heat waves.\n",
       "\n",
       "**Autumn (September - October):**\n",
       "\n",
       "*   **Pros:**\n",
       "    *   **Beautiful Foliage:** Stunning autumn colors in many regions.\n",
       "    *   **Pleasant Weather:** Mild temperatures, especially in early autumn.\n",
       "    *   **Fewer Crowds:** Tourist crowds begin to thin out after the summer rush.\n",
       "    *   **Lower Prices:** More affordable accommodation and flights.\n",
       "    *   **Harvest Season:** Wine and food festivals.\n",
       "\n",
       "*   **Cons:**\n",
       "    *   **Unpredictable Weather:** Can be rainy or chilly, especially later in autumn.\n",
       "    *   **Some Attractions May Have Limited Hours:** Particularly towards the end of October.\n",
       "\n",
       "**Winter (November - March):**\n",
       "\n",
       "*   **Pros:**\n",
       "    *   **Christmas Markets:** Festive Christmas markets throughout December.\n",
       "    *   **Skiing and Winter Sports:** Excellent skiing and snowboarding opportunities in the Alps and other mountain regions.\n",
       "    *   **Fewer Crowds:** Significantly fewer tourists than other seasons.\n",
       "    *   **Lowest Prices:** Accommodation and flights are typically the cheapest.\n",
       "    *   **Cozy Atmosphere:** Enjoy indoor activities like museums, cafes, and restaurants.\n",
       "\n",
       "*   **Cons:**\n",
       "    *   **Cold Weather:** Can be very cold, snowy, and icy, especially in Northern and Eastern Europe.\n",
       "    *   **Short Daylight Hours:** Less time to explore during the day.\n",
       "    *   **Some Attractions May Be Closed:** Reduced opening hours or closures for some attractions.\n",
       "    *   **Possible Travel Disruptions:** Snow and ice can cause travel delays and cancellations.\n",
       "\n",
       "**Here's a table summarizing the best seasons for specific interests:**\n",
       "\n",
       "| Interest          | Best Season(s)                                   |\n",
       "|-------------------|---------------------------------------------------|\n",
       "| Sightseeing       | Spring, Autumn                                     |\n",
       "| Beaches           | Summer                                            |\n",
       "| Outdoor Activities (Hiking, Cycling) | Spring, Autumn                                     |\n",
       "| Skiing            | Winter                                             |\n",
       "| Festivals         | Spring, Summer, Autumn (depending on the festival) |\n",
       "| Budget Travel     | Spring, Autumn, Winter                             |\n",
       "| Christmas Markets | Winter (December)                                 |\n",
       "\n",
       "**In conclusion:**\n",
       "\n",
       "*   **For good weather, fewer crowds, and reasonable prices:** Spring or Autumn.\n",
       "*   **For guaranteed sunshine and swimming:** Summer (be prepared for crowds and higher prices).\n",
       "*   **For Christmas Markets and skiing:** Winter.\n",
       "\n",
       "To determine the \"best\" season for *you*, consider what you want to see and do, your budget, and your tolerance for crowds and weather conditions.  Do some research on the specific regions of Europe you want to visit, as weather patterns can vary significantly.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = client.chats.create(model = \"gemini-2.0-flash\", history=[])\n",
    "response = chat.send_message(\"what is the best season to visit Europe ?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's narrow down the \"best city\" to visit in Europe. Just like the best season, the best city depends entirely on your personal preferences and what you're looking for in a trip.  To give you a more tailored recommendation, could you tell me a bit about what you enjoy? For example:\n",
       "\n",
       "*   **What are your interests?** (e.g., History, art, food, nightlife, architecture, nature, shopping, relaxation)\n",
       "*   **What's your budget like?** (e.g., Budget-friendly, mid-range, luxury)\n",
       "*   **What kind of atmosphere are you looking for?** (e.g., Bustling and energetic, charming and romantic, relaxed and peaceful)\n",
       "*   **Do you prefer a city that's easy to navigate, or are you okay with a more sprawling metropolis?**\n",
       "*   **Do you have a specific region of Europe in mind?** (e.g., Western Europe, Eastern Europe, Southern Europe, Scandinavia)\n",
       "\n",
       "In the meantime, here are a few cities that are consistently popular and cater to different interests:\n",
       "\n",
       "**For History and Culture:**\n",
       "\n",
       "*   **Rome, Italy:**  Ancient ruins, stunning churches, world-class museums (Vatican City!), delicious food, and a vibrant atmosphere.  (Can be crowded, especially in peak season.)\n",
       "*   **Athens, Greece:** Birthplace of democracy, with iconic landmarks like the Acropolis and the Parthenon. Great food and a fascinating history. (Can be very hot in summer.)\n",
       "*   **Prague, Czech Republic:**  Beautiful medieval architecture, Charles Bridge, Prague Castle, and a lively nightlife.  (Relatively affordable, but can be crowded.)\n",
       "\n",
       "**For Art and Architecture:**\n",
       "\n",
       "*   **Paris, France:** Iconic landmarks like the Eiffel Tower and the Louvre Museum, world-class art, fashion, and food. (Expensive and can be very crowded.)\n",
       "*   **Florence, Italy:**  The heart of the Renaissance, home to masterpieces by Michelangelo and Leonardo da Vinci.  Beautiful architecture and delicious Tuscan cuisine. (Crowded and can be pricey.)\n",
       "*   **Barcelona, Spain:**  Unique architecture by Antoni Gaud√≠ (Sagrada Familia, Park G√ºell), a vibrant nightlife, and beautiful beaches.  (Popular and can be crowded.)\n",
       "\n",
       "**For Food and Drink:**\n",
       "\n",
       "*   **San Sebasti√°n, Spain:** Known for its Michelin-starred restaurants and its Basque cuisine (pintxos). Beautiful beaches and a charming atmosphere. (Relatively expensive.)\n",
       "*   **Bologna, Italy:**  Nicknamed \"La Grassa\" (The Fat One) for its rich culinary traditions.  Home to delicious pasta dishes like tagliatelle al rag√π (Bolognese sauce). (Less touristy than other Italian cities.)\n",
       "*   **Lyon, France:** Considered the gastronomic capital of France, with traditional bouchons (restaurants) serving hearty Lyonnaise cuisine.\n",
       "\n",
       "**For Budget Travel:**\n",
       "\n",
       "*   **Budapest, Hungary:**  Beautiful architecture, thermal baths, ruin bars, and affordable prices.  (Becoming increasingly popular, but still relatively affordable.)\n",
       "*   **Krak√≥w, Poland:**  A historic city with a charming Old Town, Wawel Castle, and poignant reminders of World War II (Auschwitz-Birkenau nearby).  (Very affordable.)\n",
       "*   **Lisbon, Portugal:**  A vibrant city with colorful buildings, historic neighborhoods, delicious seafood, and affordable prices.  (Hilly terrain, but beautiful views.)\n",
       "\n",
       "**For a Unique Experience:**\n",
       "\n",
       "*   **Reykjavik, Iceland:** Stunning natural landscapes, including glaciers, volcanoes, and the Northern Lights.  (Expensive and the weather can be unpredictable.)\n",
       "*   **Amsterdam, Netherlands:** Canals, bicycles, museums, and a liberal atmosphere. (Can be crowded and expensive.)\n",
       "*   **Berlin, Germany:**  A city with a rich history, vibrant arts scene, and a unique blend of cultures. (Relatively affordable and easy to get around.)\n",
       "\n",
       "Give me a little more information about what you're looking for, and I can give you a more personalized recommendation!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"What is the best city to visit then ?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your first question was: \"what is the best season to visit Europe ?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Do you know what was my first question ?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 : A legacy text-only model optimized for chat conversations\n",
      "models/text-bison-001 : A legacy model that understands text and generates text as an output\n",
      "models/embedding-gecko-001 : Obtain a distributed representation of a text.\n",
      "models/gemini-1.0-pro-vision-latest : The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "models/gemini-pro-vision : The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "models/gemini-1.5-pro-latest : Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "models/gemini-1.5-pro-001 : Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "models/gemini-1.5-pro-002 : Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "models/gemini-1.5-pro : Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "models/gemini-1.5-flash-latest : Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "models/gemini-1.5-flash-001 : Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "models/gemini-1.5-flash-001-tuning : Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "models/gemini-1.5-flash : Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "models/gemini-1.5-flash-002 : Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "models/gemini-1.5-flash-8b : Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "models/gemini-1.5-flash-8b-001 : Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "models/gemini-1.5-flash-8b-latest : Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "models/gemini-1.5-flash-8b-exp-0827 : Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "models/gemini-1.5-flash-8b-exp-0924 : Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "models/gemini-2.5-pro-exp-03-25 : Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "models/gemini-2.5-pro-preview-03-25 : Gemini 2.5 Pro Preview 03-25\n",
      "models/gemini-2.0-flash-exp : Gemini 2.0 Flash Experimental\n",
      "models/gemini-2.0-flash : Gemini 2.0 Flash\n",
      "models/gemini-2.0-flash-001 : Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "models/gemini-2.0-flash-exp-image-generation : Gemini 2.0 Flash (Image Generation) Experimental\n",
      "models/gemini-2.0-flash-lite-001 : Stable version of Gemini 2.0 Flash Lite\n",
      "models/gemini-2.0-flash-lite : Gemini 2.0 Flash-Lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05 : Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "models/gemini-2.0-flash-lite-preview : Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "models/gemini-2.0-pro-exp : Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "models/gemini-2.0-pro-exp-02-05 : Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "models/gemini-exp-1206 : Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 : Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "models/gemini-2.0-flash-thinking-exp : Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "models/gemini-2.0-flash-thinking-exp-1219 : Gemini 2.0 Flash Thinking Experimental\n",
      "models/learnlm-1.5-pro-experimental : Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "models/gemma-3-1b-it : None\n",
      "models/gemma-3-4b-it : None\n",
      "models/gemma-3-12b-it : None\n",
      "models/gemma-3-27b-it : None\n",
      "models/embedding-001 : Obtain a distributed representation of a text.\n",
      "models/text-embedding-004 : Obtain a distributed representation of a text.\n",
      "models/gemini-embedding-exp-03-07 : Obtain a distributed representation of a text.\n",
      "models/gemini-embedding-exp : Obtain a distributed representation of a text.\n",
      "models/aqa : Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "models/imagen-3.0-generate-002 : Vertex served Imagen 3.0 002 model\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(f\"{model.name} : {model.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_output_tokens** : Parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Algorithmic Revolution: Examining the Potential and Perils of Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) has transitioned from the realm of science fiction to a tangible and transformative force reshaping nearly every facet of modern life. From personalized recommendations on streaming services to complex algorithms powering medical diagnoses, AI's influence is undeniable and rapidly expanding. This essay will explore the multifaceted nature of AI, examining its diverse applications, the ethical dilemmas it presents, and the potential societal impacts, both positive and negative, that lie ahead.\n",
      "\n",
      "At its core, AI encompasses the development of computer systems capable of performing tasks that typically require human intelligence. This includes learning, problem-solving, perception, understanding natural language, and decision-making. The field is broadly categorized into two main branches: Narrow or Weak AI and General or Strong AI. Narrow AI, which dominates the current landscape, is designed to perform specific tasks with exceptional efficiency. Examples include image recognition software, spam filters, and virtual assistants like Siri and Alexa.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "token_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = token_config,\n",
    "    contents = \"Give me a 1000 word essay on AI.\"\n",
    "    )\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature** : **0-2** : This parameter controls the degree of randomness in token selection. Higher value for the temperature produces the randomness on how the words are seleceted like value of 2 gives model more words to choose from producing the unexpected ouput but on the other hand value of 0 is greedy decoding that will choose the word with the most probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lavender\n",
      "\n",
      "Fuchsia\n",
      "\n",
      "Cerulean\n",
      "\n",
      "Azure\n",
      "\n",
      "Burgundy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.0-flash\",\n",
    "        config = high_temp_config,\n",
    "        contents = \"Give me a random color ....... (Respond in a single word)\"\n",
    "    )\n",
    "    if response.text:\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top_P** : **0-1** : It defines the cap probability threshold for tokens. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clementine, a calico of discerning tastes and a perpetually unimpressed expression, considered the dust bunny under the sofa. It was, she decided, profoundly boring. Her usual routine of napping, demanding tuna, and judging the neighbor's poodle was losing its luster. Clementine needed‚Ä¶ adventure.\n",
      "\n",
      "The opportunity presented itself in the form of a carelessly left-open window. A breeze, scented with honeysuckle and something wilder, something untamed, tickled her whiskers. Clementine, abandoning all pretense of domesticity, leaped onto the windowsill.\n",
      "\n",
      "The world exploded with new sensations. The rough bark of the oak tree scraped against her paws as she climbed down. The air buzzed with the frantic energy of insects. The scent of damp earth and decaying leaves filled her nostrils. This was far more interesting than tuna.\n",
      "\n",
      "Her adventure began in the overgrown garden. She stalked a plump bumblebee, its buzzing a challenge. She batted at a dew-kissed spiderweb, its silken threads clinging to her fur. She even, with a dramatic hiss, confronted a particularly arrogant earthworm.\n",
      "\n",
      "Beyond the garden lay the woods. Sunlight dappled through the leaves, creating shifting patterns of light and shadow. Clementine, emboldened by her garden conquests, ventured deeper. She followed a winding path, her tail held high like a proud banner.\n",
      "\n",
      "She encountered a grumpy squirrel who chattered insults from a high branch. She navigated a babbling brook, carefully stepping from stone to stone. She even, for a fleeting moment, caught the scent of a fox, a thrill of primal fear and excitement coursing through her.\n",
      "\n",
      "As dusk began to paint the sky in hues of orange and purple, Clementine found herself in a clearing. A firefly blinked its tiny light, a beacon in the growing darkness. She felt a pang of something she couldn't quite name, a mixture of loneliness and satisfaction.\n",
      "\n",
      "Suddenly, a familiar voice called out, \"Clementine! Clementine, where are you?\"\n",
      "\n",
      "It was her human, Sarah, her voice laced with worry. Clementine hesitated. Should she stay, embrace the wild, become a creature of the woods? Or should she return to the comfort of her familiar life?\n",
      "\n",
      "The scent of tuna, carried on the evening breeze, made the decision for her.\n",
      "\n",
      "With a soft meow, Clementine emerged from the trees. Sarah rushed towards her, scooping her into a hug. Clementine purred, burying her face in Sarah's hair.\n",
      "\n",
      "Back inside, curled up on her favorite cushion, Clementine licked her paws, a faint scent of earth and adventure clinging to her fur. The tuna tasted even better than usual.\n",
      "\n",
      "The dust bunny under the sofa still looked profoundly boring. But now, Clementine knew, even the most mundane life could hold the promise of adventure, just waiting for an open window and a curious heart. And perhaps, tomorrow, she would explore the attic. After a nap, of course.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_p_config = types.GenerateContentConfig(top_p = 0.5, temperature= 0.5)\n",
    "\n",
    "text = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = top_p_config,\n",
    "    contents = text\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-Shot** : Zero-shot prompts are prompts that describe the request for the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature = 0.2,\n",
    "    top_p = 1,\n",
    "    max_output_tokens= 5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "                    Review: \"Her\" is a disturbing study revealing the direction\n",
    "                    humanity is headed if AI is allowed to keep evolving,\n",
    "                    unchecked. I wish there weren't more movies like this masterpiece.\n",
    "                    Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = model_config,\n",
    "    contents = zero_shot_prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not good\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"not good\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Shot and Few-Shot** : Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250\n",
    "    ),\n",
    "    contents = [few_shot_prompt, customer_order]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought (COT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1. **Find the age difference:** When you were 4, your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n",
       "\n",
       "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3. **Determine the partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Therefore, your partner is now 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is now 28 years old.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and look at the authors to see who the youngest one is.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences= [\"\\nObservation\"],\n",
    "    system_instruction= model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "react_chat = client.chats.create(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = react_config\n",
    ")\n",
    "\n",
    "response = react_chat.send_message(question)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "I have the authors listed: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest of these people.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani age</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "response_2 = react_chat.send_message(observation)\n",
    "print(response_2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information available, the youngest author listed on the \"Attention is All You Need\" paper (the seminal paper that introduced the Transformer architecture in NLP) is likely **Aidan N. Gomez**.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Aidan N. Gomez** was a PhD student at the University of Oxford at the time of the paper's publication (2017).  PhD students are generally younger than more established researchers and professors who typically lead research projects.\n",
       "* The other authors, such as Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, ≈Åukasz Kaiser, and Illia Polosukhin, were mostly researchers at Google Brain or the University of Toronto, and were likely further along in their careers than a PhD student.\n",
       "\n",
       "While we don't have exact birthdates for all authors to definitively confirm this, being a PhD student at the time of publication strongly suggests Aidan N. Gomez was the youngest author on the paper.\n",
       "\n",
       "Therefore, the answer is likely **Aidan N. Gomez**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io # for creating the fake files\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()  # Created a Fake file while not using the disk but only ram\n",
    "\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n",
      "         'prime numbers. Remember that a prime number is a natural number '\n",
      "         'greater than 1 that has no positive divisors other than 1 and '\n",
      "         'itself. The first few prime numbers are 2, 3, 5, 7, 11, and so on. '\n",
      "         \"Since I need the odd primes, I'll exclude 2. After generating these, \"\n",
      "         \"I'll calculate their sum.\\n\"\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
      "                             '37, 41, 43, 47]\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "-----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers, then calculate their sum.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can do that. First, I need to generate the first 14 odd prime numbers. Remember that a prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. The first few prime numbers are 2, 3, 5, 7, 11, and so on. Since I need the odd primes, I'll exclude 2. After generating these, I'll calculate their sum.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{primes=}')\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
